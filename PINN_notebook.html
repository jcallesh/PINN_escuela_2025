<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>pinn_notebook ‚Äì PINNs - Escuela 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3c73a2198eb6a3db9d40c404f225a8bf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">PINNs - Escuela 2025</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Inicio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./intro.html"> 
<span class="menu-text">PINN Intro</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./colab.html"> 
<span class="menu-text">Colab</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./PINN_notebook.html" aria-current="page"> 
<span class="menu-text">Hands-on</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#requisitos-y-ejecuci√≥n" id="toc-requisitos-y-ejecuci√≥n" class="nav-link active" data-scroll-target="#requisitos-y-ejecuci√≥n"><span class="header-section-number">1</span> Requisitos y ejecuci√≥n</a>
  <ul class="collapse">
  <li><a href="#punto-de-partida-necesitamos-datos" id="toc-punto-de-partida-necesitamos-datos" class="nav-link" data-scroll-target="#punto-de-partida-necesitamos-datos"><span class="header-section-number">1.1</span> üî¢ Punto de partida: ¬°Necesitamos datos!</a></li>
  <li><a href="#red-neuronal" id="toc-red-neuronal" class="nav-link" data-scroll-target="#red-neuronal"><span class="header-section-number">1.2</span> üï∏Ô∏è Red Neuronal</a></li>
  <li><a href="#autograd" id="toc-autograd" class="nav-link" data-scroll-target="#autograd"><span class="header-section-number">1.3</span> ‚öôÔ∏è <strong>Autograd</strong></a></li>
  </ul></li>
  <li><a href="#ejemplo-guiado-ecuaci√≥n-de-poisson-1d" id="toc-ejemplo-guiado-ecuaci√≥n-de-poisson-1d" class="nav-link" data-scroll-target="#ejemplo-guiado-ecuaci√≥n-de-poisson-1d"><span class="header-section-number">2</span> <span style="color:#1f77b4;">Ejemplo guiado:</span> Ecuaci√≥n de Poisson 1D</a>
  <ul class="collapse">
  <li><a href="#extras" id="toc-extras" class="nav-link" data-scroll-target="#extras"><span class="header-section-number">2.0.1</span> Extras:</a></li>
  </ul></li>
  <li><a href="#ejemplo-guiado-oscilador-arm√≥nico-en-1d" id="toc-ejemplo-guiado-oscilador-arm√≥nico-en-1d" class="nav-link" data-scroll-target="#ejemplo-guiado-oscilador-arm√≥nico-en-1d"><span class="header-section-number">3</span> <span style="color:#1f77b4;">Ejemplo guiado:</span> Oscilador arm√≥nico en 1D</a>
  <ul class="collapse">
  <li><a href="#extra" id="toc-extra" class="nav-link" data-scroll-target="#extra"><span class="header-section-number">3.0.1</span> Extra:</a></li>
  </ul></li>
  <li><a href="#ejemplo-guiado-ecuaci√≥n-de-helmholtz" id="toc-ejemplo-guiado-ecuaci√≥n-de-helmholtz" class="nav-link" data-scroll-target="#ejemplo-guiado-ecuaci√≥n-de-helmholtz"><span class="header-section-number">4</span> <span style="color:#1f77b4;">Ejemplo guiado:</span> Ecuaci√≥n de Helmholtz</a></li>
  <li><a href="#case-of-study-poisson-en-2d" id="toc-case-of-study-poisson-en-2d" class="nav-link" data-scroll-target="#case-of-study-poisson-en-2d"><span class="header-section-number">5</span> <span style="color:#1f77b4;">Case of study:</span> Poisson en 2D</a></li>
  <li><a href="#case-of-study-laplace-en-2d-con-una-singularidad-de-cu√±a" id="toc-case-of-study-laplace-en-2d-con-una-singularidad-de-cu√±a" class="nav-link" data-scroll-target="#case-of-study-laplace-en-2d-con-una-singularidad-de-cu√±a"><span class="header-section-number">6</span> <span style="color:#1f77b4;">Case of study:</span> Laplace en 2D con una singularidad de cu√±a</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><h1 style="color:#F14925; font-size:2.5em; margin-bottom:0.3em;" class="title display-7">
Physics-Informed Neural Networks (PINNs)
</h1></header>




<div style="text-align:center; font-family:sans-serif; line-height:1.6;">

<p style="font-size:1.8em; margin:0.2em 0;">
Juan Calles - <a href="mailto:juan.calles@uv.cl" style="color:inherit; text-decoration:none;">juan.calles@uv.cl</a>
</p>
<p style="font-size:1.5em; font-weight:bold; margin-top:1em;">
Escuela de F√≠sica Te√≥rica de Valpara√≠so
</p>
<p style="font-size:1.3em; margin:0.2em 0;">
3 - 4 - 5 de Diciembre de 2025
</p>
<p style="font-size:1.3em; margin:0.2em 0;">
Facultad de Ciencias y Edificio CIAE
</p>
<p style="font-size:1.3em; margin:0.2em 0;">
Universidad de Valpara√≠so
</p>
</div>
<section id="requisitos-y-ejecuci√≥n" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Requisitos y ejecuci√≥n</h1>
<p>En esta sesi√≥n utilizaremos <code>PyTorch</code> para construir y entrenar redes neuronales. Asumiremos que ya tienes familiaridad con las siguientes herramientas:</p>
<ul>
<li>Github</li>
<li>Python</li>
<li>Jupyter notebooks</li>
<li>Numpy, Matplotlib, scipy</li>
<li>pytorch</li>
</ul>
<p>Existen, sin embargo, otras alternativas ampliamente utilizadas en investigaci√≥n y la industria:</p>
<ol type="1">
<li><a href="https://www.tensorflow.org/">TensorFlow/Keras</a></li>
<li><a href="https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html">JAX</a></li>
<li><a href="https://sciml.ai/">SciML-Julia</a></li>
</ol>
<p>Adem√°s de las herramientas mencionadas, este notebook asume que tienes nociones b√°sicas de lo siguiente:</p>
<ul>
<li>Neural network (NN)</li>
<li>Multilayer Perceptron (MLP)</li>
<li>Entrenamiento de una NN -&gt; Regresi√≥n/Clasificaci√≥n</li>
</ul>
<div id="99c1c3dc" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    torch.set_default_device(<span class="st">"cuda"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="punto-de-partida-necesitamos-datos" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="punto-de-partida-necesitamos-datos"><span class="header-section-number">1.1</span> üî¢ Punto de partida: ¬°Necesitamos datos!</h2>
<p>Para comenzar cualquier proceso de modelado o an√°lisis, lo primero que necesitamos son datos!</p>
<p>T√≠picamente, estos se presentan como:</p>
<ul>
<li><strong>Entradas (inputs)</strong>: <span class="math inline">\(x_{1:N}=(x_1,x_2,...,x_N)\)</span>
<ul>
<li>donde cada <span class="math inline">\(x_i\)</span> representa una observaci√≥n o conjunto de caracter√≠sticas.</li>
</ul></li>
<li><strong>Salidas (outputs)</strong>: <span class="math inline">\(y_{1:N}=(y_1,y_2,...,y_N)\)</span>
<ul>
<li>que corresponden a las respuestas, etiquetas o valores asociados a cada entrada.</li>
</ul></li>
</ul>
<p>Este par <span class="math inline">\(\mathcal{D} = \{x_i,\, y_i\}_{i=1}^{N}\)</span> forma lo que llamamos una muestra (<em>sample</em>) de entrenamiento, y el conjunto completo de pares nos permite aprender patrones estad√≠sticos, ajustar modelos, o validar hip√≥tesis que <strong>expliquen el proceso que generaron los outputs</strong>!.</p>
<p>üéØ Objetivo: simular una se√±al tipo seno con ruido gaussiano</p>
<p>Antes de aplicar cualquier modelo, vamos a generar nuestros propios datos. Estos datos ser√°n <strong>sint√©ticos</strong>, es decir, <strong>creados artificialmente</strong> en lugar de recolectados del mundo real.</p>
<p>¬øPor qu√© usar datos sint√©ticos?</p>
<ul>
<li>Control total: Podemos definir exactamente la estructura, el ruido (noise) y las <strong>propiedades estad√≠sticas</strong>.</li>
<li>Validaci√≥n de modelos: Son ideales para probar si un modelo puede recuperar patrones conocidos.</li>
<li>Diagn√≥stico pedag√≥gico: Permiten ilustrar conceptos sin depender de datos externos o complicaciones reales.</li>
</ul>
<p>Vamos a generar <span class="math inline">\(N\)</span> puntos de entrada <span class="math inline">\(x_i\)</span> distribuidos uniformemente en un intervalo (por ejemplo, <span class="math inline">\([0, 2\pi]\)</span>), y para cada uno calcularemos una salida <span class="math inline">\(y_i\)</span> como:</p>
<p><span class="math display">\[y_i = \sin(x_i) + \epsilon_i\]</span></p>
<p>donde <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span> es ruido Gaussiano/Normal con media cero y varianza <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Esto nos da un conjunto de datos <span class="math inline">\(\{x_i,\, y_i\}\)</span> que <strong>emula</strong> una se√±al peri√≥dica con perturbaciones aleatorias.</p>
<div id="c943e235" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Completar la siguiente funci√≥n para que genere un conjunto de datos sint√©ticos que emule una distribuci√≥n seno + ruido gaussiano</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generar_datos(num_puntos, desviacion_ruido<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generar valores aleatorios para x</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    valores_x <span class="op">=</span> np.sort(np.random.uniform(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> np.pi, num_puntos))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generar los correspondientes valores de y = sen(x) + N(0, desviaci√≥n)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    valores_y <span class="op">=</span> np.sin(valores_x) <span class="op">+</span> np.random.normal(<span class="dv">0</span>, desviacion_ruido, num_puntos)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> valores_x, valores_y</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>num_puntos <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Agregue un par√°metro a la funci√≥n que module el nivel de ruido de los datos atraves de la desviaci√≥n estandard sigma</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>x_train, y_train<span class="op">=</span> generar_datos(num_puntos, desviacion_ruido<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Antes de entrenar cualquier modelo, lo m√°s importante es visualizar y entender c√≥mo lucen nuestros datos. Esto nos permite detectar patrones, outliers, ruido o posibles errores de generaci√≥n.</p>
<div style="border-left: 4px solid #4caf50; padding: 8px;">
<p>üí° <strong>Nota:</strong> Cuando los datos son ‚Äúsint√©ticos‚Äù tenemos la ventaja de conocer la distribuci√≥n exacta que deber√≠an seguir.</p>
</div>
<div id="b82f13e4" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gr√°fico simple de los datos vs. la funci√≥n real</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_train, y_train, label<span class="op">=</span><span class="st">'Datos generados con ruido'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x_train, np.sin(x_train), color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>,label<span class="op">=</span><span class="st">'Curva seno verdadera'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Datos generados de onda seno con ruido'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valores de X'</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Valores de Y'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="PINN_notebook_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="red-neuronal" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="red-neuronal"><span class="header-section-number">1.2</span> üï∏Ô∏è Red Neuronal</h2>
<p>Nuestro objetivo ahora es construir una red neuronal capaz de <strong>aprender</strong> a ajustar los datos sint√©ticos generados previamente. Para ello, implementaremos un Perceptr√≥n Multicapa (MLP, <em>Multilayer Perceptron</em>), que es la forma m√°s b√°sica de red neuronal.</p>
<blockquote class="blockquote">
<p>Un MLP est√° formado por varias capas lineales (<em>fully connected layers</em>), donde cada neurona se conecta con todas las de la capa anterior.</p>
<p>Entre estas capas se insertan funciones de activaci√≥n que introducen <em>no linealidad</em>, permitiendo que la red aprenda relaciones complejas entre entrada y salida.</p>
</blockquote>
<p>En t√©rminos generales, nuestro modelo puede entenderse como una funci√≥n, <span class="math inline">\(\hat{y}\)</span>, parametrizada por los pesos(weights) y sesgos(biases) de la red <span class="math inline">\((w_{ij}, b_i)\)</span>, que agrupamos bajo el s√≠mbolo <span class="math inline">\(\theta\)</span>. As√≠, la red <span class="math inline">\(\hat{y}_\theta\)</span> representa una funci√≥n que depende de estos par√°metros, toma como entrada los valores <span class="math inline">\(x_i\)</span>, y produce como salida las predicciones correspondientes.</p>
<p>Arquitectura propuesta</p>
<ul>
<li><strong>Capa 1:</strong> Lineal, transforma las entradas en una representaci√≥n de dimensi√≥n intermedia.</li>
<li><strong>Funci√≥n de activaci√≥n:</strong> puede ser <code>ReLU</code> (m√°s simple y r√°pida) o <code>Tanh</code> (m√°s suave y √∫til para funciones continuas).</li>
<li><strong>Capa 2:</strong> Lineal, genera la salida final del modelo.</li>
</ul>
<p>Ayuda:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="bu">input</span>,output)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(<span class="bu">input</span>,output)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    nn.functional.tanh</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    nn.function.relu</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    nn.function.softmax</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div id="8341769a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MLP, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>)          <span class="co"># Fully connected layer: 1 input feature, 10 hidden units</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.functional.tanh <span class="co"># Funci√≥n de activaci√≥n</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)          <span class="co"># Fully connected layer: 10 hidden units, 1 output</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(x)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>üß† ¬øC√≥mo entrenamos nuestro modelo?</p>
<p>‚Äú<em>Aprender</em>‚Äù significa ajustar los par√°metros internos de la red, denotados por <span class="math inline">\(\theta\)</span>, para que las predicciones <span class="math inline">\(\hat{y}_i\)</span> se acerquen lo m√°s posible a los valores reales <span class="math inline">\(y_i\)</span>. Este proceso se basa en dos componentes esenciales:</p>
<ol type="1">
<li><p><strong>Minimizaci√≥n de una funci√≥n de costo (Loss function):</strong></p>
<p>Para medir qu√© tan bien est√° funcionando el modelo, usamos una funci√≥n de costo que cuantifica el error entre las predicciones y los datos reales. Una de las m√°s comunes es el Error Cuadr√°tico Medio (MSE):</p>
<p><span class="math display">\[
\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \left( y_i - \hat{y}_i \right)^2
\]</span> donde <span class="math inline">\(y_i\)</span> es el valor real y <span class="math inline">\(\hat{y}_i\)</span> es la predicci√≥n del modelo para la entrada <span class="math inline">\(x_i\)</span>. &gt; Notenemos que necesitamos 3 argumentos <span class="math inline">\(\{x_i,\hat{y}(x_i),y(x_i)\}\)</span> para evaluar esta funci√≥n!</p></li>
<li><p><strong>Actualizaci√≥n por descenso del gradiente:</strong></p>
<p>Para encontrar los valores √≥ptimos de los par√°metros del modelo, <span class="math inline">\(\theta\)</span>, que minimizan la funci√≥n de costo, <span class="math inline">\(\mathcal{L}\)</span>, se utiliza el m√©todo de descenso del gradient (<em>gradient descent</em>). La idea central es mover los par√°metros en la direcci√≥n opuesta al gradiente de la p√©rdida, es decir, en la direcci√≥n que m√°s reduce el error:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}
\]</span></p>
<p>donde:</p>
<ul>
<li><p><span class="math inline">\(\eta\)</span> es la tasa de aprendizaje (<em>learning rate</em>), que controla qu√© tan grande es cada paso de actualizaci√≥n.</p></li>
<li><p><span class="math inline">\(\nabla_\theta \mathcal{L}\)</span> es el gradiente de la p√©rdida respecto a los par√°metros del modelo.</p>
<ul>
<li>Este gradiente indica la direcci√≥n de mayor aumento de la p√©rdida; por eso se resta en la actualizaci√≥n.</li>
<li>En la pr√°ctica, los gradientes se calculan mediante <strong>diferenciaci√≥n autom√°tica</strong> (<em>automatic differentiation</em>) en librer√≠as como <em>PyTorch</em> o <em>TensorFlow</em>.</li>
</ul></li>
</ul>
<p>Existen distintos optimizadores que implementan variantes del descenso del gradiente:</p>
<ul>
<li><strong>SGD (Stochastic Gradient Descent)</strong>: usa un subconjunto aleatorio de datos en cada iteraci√≥n.</li>
<li><strong>RMSProp</strong>: ajusta din√°micamente el tama√±o de paso para cada par√°metro seg√∫n la magnitud reciente de sus gradientes.</li>
<li><strong>Adam</strong>: combina las ideas de <em>momentum</em> y escalamiento adaptativo de tasa de aprendizaje, siendo hoy el m√°s popular por su estabilidad y rapidez de convergencia.</li>
</ul></li>
</ol>
<p>Este ciclo: <em>calcular la p√©rdida ‚Üí obtener los gradientes ‚Üí actualizar par√°metros</em>, se repite iterativamente hasta que:</p>
<ul>
<li>la p√©rdida sea lo suficientemente baja,</li>
<li>el modelo converja, o</li>
<li>se alcance el n√∫mero m√°ximo de iteraciones.</li>
</ul>
<div style="border-left: 4px solid #4caf50; padding: 8px;">
<p>üí° <strong>En t√©rminos simples:</strong> el modelo aprende a ajustar <span class="math inline">\(\theta\)</span>, repitiendo peque√±os pasos en la direcci√≥n ‚Äúcorrecta‚Äù, hasta que <span class="math inline">\(\hat{y}_\theta\)</span> se ajusta a los datos.</p>
</div>
<div id="09d5f6b8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ahora definimos en Pytorch nuestro c√≥digo para entrenar la red:</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(inputs, model, targets, optimizer, criterion):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(inputs)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ademas agregamos una peque√±a funci√≥n que nos ayuda a evaluar el model</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, x_test):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(x_test).cpu().numpy()</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_pred.squeeze()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="902a5c19" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Iniciamos las instancias correspondientes</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>modelo       <span class="op">=</span> MLP()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>criterio     <span class="op">=</span> nn.MSELoss()</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>optimizador  <span class="op">=</span> optim.Adam(modelo.parameters(), lr<span class="op">=</span><span class="fl">1e-2</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">#numero de parametros de la red</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>numero_parametros <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> modelo.parameters())</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of parameters: </span><span class="sc">{</span>numero_parametros<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of parameters: 31</code></pre>
</div>
</div>
<div id="dc308479" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Y ahora definimos el loop 'iterativo' de entrenamiento</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>num_epocas <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoca <span class="kw">in</span> <span class="bu">range</span>(num_epocas):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convertimos los datos a Tensores de pytorch</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    x_i  <span class="op">=</span> torch.tensor(x_train[:,<span class="va">None</span>], dtype<span class="op">=</span>torch.float32)  <span class="co">#inputs shape (Npuntos,1) </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.tensor(y_train[:,<span class="va">None</span>], dtype<span class="op">=</span>torch.float32)     <span class="co">#targets shape (Npuntos,1) </span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Entrenamos</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> train_epoch(x_i, modelo, y, optimizador, criterio)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoca<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch [</span><span class="sc">{</span>epoca<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epocas<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [100/1000], Loss: 0.0941
Epoch [200/1000], Loss: 0.0522
Epoch [300/1000], Loss: 0.0293
Epoch [400/1000], Loss: 0.0164
Epoch [500/1000], Loss: 0.0117
Epoch [600/1000], Loss: 0.0107
Epoch [700/1000], Loss: 0.0105
Epoch [800/1000], Loss: 0.0104
Epoch [900/1000], Loss: 0.0104
Epoch [1000/1000], Loss: 0.0103</code></pre>
</div>
</div>
<p>Ya tenemos nuestro modelo entrenado! Pero‚Ä¶ <strong>¬ørealmente aprendi√≥ algo √∫til?</strong></p>
<p>Para responderlo, haremos una inspecci√≥n visual de las predicciones <span class="math inline">\(\hat{y}_i\)</span> generadas por la red.</p>
<p>üìà Una visualizaci√≥n t√≠pica incluye:</p>
<ol type="1">
<li><strong>Datos de entrenamiento:</strong> los puntos originales <span class="math inline">\((x_i, y_i)\)</span>, que representan nuestras observaciones.</li>
<li><strong>Predicci√≥n del modelo:</strong> la funci√≥n <span class="math inline">\(\hat{y}(x_{\text{new}})\)</span> evaluada en un conjunto de puntos nuevos y m√°s densos, para ver la forma continua que aprendi√≥.</li>
<li><strong>Soluci√≥n real (si la conocemos):</strong> <span class="math inline">\(y(x_{\text{new}})\)</span> evaluada en los mismos puntos del paso anterior, para comparar con la salida de la red.</li>
</ol>
<div id="6d5cf234" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, <span class="dv">100</span>) <span class="co"># x_nuevos</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> evaluate(modelo, x_test) <span class="co"># Nuestro modelo evaluado en los nuevos puntos x</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> x_test.cpu().numpy()</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> np.sin(x_test) <span class="co"># Funcion verdadera evaluada en los nuevos puntos x_new</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_train, y_train, label<span class="op">=</span><span class="st">'Datos de entrenamiento'</span>,alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x_test, y_pred, label<span class="op">=</span><span class="st">'Curva aprendida'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>plt.plot(x_test, y_test, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>,label<span class="op">=</span><span class="st">'Curva seno verdadera'</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="PINN_notebook_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>üîç ¬øQu√© podemos observar de este gr√°fico?</p>
<ul>
<li>Si el modelo captura correctamente la forma general de la funci√≥n (por ejemplo, una senoide).</li>
<li>Si existen zonas sistem√°ticamente mal ajustadas.</li>
<li>Si la predicci√≥n es suave, ruidosa o err√°tica.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Extra para pensar:</strong><br>
- ¬øQu√© sucede si incrementamos <span class="math inline">\(\sigma\)</span> en nuestro generador de datos?<br>
- ¬øC√≥mo se comporta nuestro modelo para valores m√°s altos o m√°s bajos de <span class="math inline">\(\sigma\)</span>?</p>
</blockquote>
<p>Otro tipo de visualizaci√≥n implica gr√°ficar la curva de la funci√≥n de coste versus el n√∫mero de √©pocas.</p>
<p>Para ello, modifiqu√© la rutina de entrenamiento para que guarde el valor de la p√©rdida en cada √©poca y luego grafique <code>loss vs epochs</code>.</p>
<div id="60a90b03" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># num_epochs = 1000</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># for epoch in range(num_epochs):</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">#     # Convertimos los datos a Tensores de pytorch</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">#     inputs  = torch.tensor(x_train, dtype=torch.float32, requires_grad=True).view(-1, 1) #esto cambien nuestros datos a un tensor de dim (Npuntos,1)</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     targets = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">#     # Entrenamos</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">#     loss = train_epoch(inputs, modelo,targets, optimizador, function_coste)</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">#     if (epoch+1) % 100 == 0:</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="a61e091d" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(lista_epocas, lista_costos, lw=2, label = 'Loss entrenamiento')</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.xlabel('Epocas')</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.ylabel('MSE_loss')</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.legend()</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>üîç ¬øQu√© podemos observar de este gr√°fico? - Verificar si el modelo est√° convergiendo. - Detectar oscilaciones o problemas con la tasa de aprendizaje (learning rate, <span class="math inline">\(\eta\)</span>). - Identificar ‚Äúoverfitting‚Äù si la p√©rdida se estabiliza pero el modelo no mejora</p>
</section>
<section id="autograd" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="autograd"><span class="header-section-number">1.3</span> ‚öôÔ∏è <strong>Autograd</strong></h2>
<p>Ahora que ya tenemos nuestro modelo entrenado, podemos hacer algo muy √∫til: calcular derivadas de la salida de la red con respecto a sus entradas.</p>
<p>Esto es posible gracias a una herramienta fundamental en PyTorch llamada <code>autograd</code>.</p>
<p>En otras palabras, podemos evaluar: <span class="math display">\[\left.\dfrac{d}{dx}\hat{y}(x)\right|_{x_i}\]</span> de forma autom√°tica, sin necesidad de derivar la red ‚Äúa mano‚Äù.</p>
<div id="b4323102" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear un conjunto de inputs con gradientes habilitados</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> np.pi, <span class="dv">100</span>).requires_grad_(<span class="va">True</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener las predicciones del modelo</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>predicciones <span class="op">=</span> modelo(x_test[:,<span class="va">None</span>])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular la derivada de las predicciones respecto a los inputs</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>dy_dx <span class="op">=</span> torch.autograd.grad(</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>predicciones,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>x_test,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    grad_outputs<span class="op">=</span>torch.ones_like(predicciones),</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    create_graph<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>)[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Veamos que tan buena es nuestra derivada</p>
<div id="7ee36ad7" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Asegurarse de que x y dy_dx est√©n en formato NumPy para usar en matplotlib.plt</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x_test.detach().cpu().numpy()</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>dy_dx_np <span class="op">=</span> dy_dx.detach().cpu().numpy()</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficar derivada real y derivada aprendida</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x, np.cos(x), label<span class="op">=</span><span class="st">'Derivada real (cos(x))'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x, dy_dx_np, label<span class="op">=</span><span class="st">'Derivada autograd del modelo'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Est√©tica del gr√°fico</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparaci√≥n entre derivada real y derivada aprendida'</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'dy/dx'</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="PINN_notebook_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Ejercicios extra:</strong></p>
<ul>
<li>Experimenta con el modelo para explorar c√≥mo responde a distintos escenarios:
<ol type="1">
<li>Para una funci√≥n rapidamente oscilante <span class="math display">\[\sin\left(\frac{2n\pi x}{L}\right)\]</span> con <span class="math inline">\(L=1\)</span> y diferentes valores de <span class="math inline">\(n=5,20,50\)</span>.</li>
<li>Qu√© sucede con el modelo?</li>
<li>Qu√© rol juega la capacidad del modelo (n√∫mero de neuronas, activaciones, n√∫mero de capas)</li>
<li>C√≥mo afecta el tama√±o del dataset y la densidad de puntos?</li>
</ol></li>
<li>¬øC√≥mo se podr√≠a calcular la segunda derivada de esta funci√≥n utilizando autograd en PyTorch?</li>
</ul>
<p><span class="math display">\[ \dfrac{d}{dx} \left(\dfrac{d}{dx}\hat{y}(x)\right)\]</span></p>
</blockquote>
<div id="6a442ac0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>d2y_dx2 <span class="op">=</span> torch.autograd.grad(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>dy_dx,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>x_test,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    grad_outputs<span class="op">=</span>torch.ones_like(dy_dx),</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>)[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b086dca7" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Asegurarse de que x y dy_dx est√©n en formato NumPy</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>d2y_dx2_np <span class="op">=</span> d2y_dx2.detach().cpu().numpy()</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficar derivada real y derivada aprendida</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x, <span class="op">-</span>np.sin(x), label<span class="op">=</span><span class="st">'Segunda Derivada (-sin(x))'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x, d2y_dx2_np, label<span class="op">=</span><span class="st">'Segunda Derivada autograd del modelo'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Est√©tica del gr√°fico</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparaci√≥n entre derivada real y autograd'</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'d2y/dx2'</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="PINN_notebook_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="ejemplo-guiado-ecuaci√≥n-de-poisson-1d" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> <span style="color:#1f77b4;">Ejemplo guiado:</span> Ecuaci√≥n de Poisson 1D</h1>
<p>Queremos resolver la ecuaci√≥n de Poisson:</p>
<p><span class="math display">\[-\frac{d^2 V}{dx^2} = \rho(x), \quad x \in [0, L] \;, \]</span> con las siguientes condiciones de borde:</p>
<p><span class="math display">\[V(0) = V(L) = 0 \, .\]</span></p>
<p>Elegimos una fuente gaussiana:</p>
<p><span class="math display">\[\rho(x) = \exp\left[ -\frac{(x - x_o)^2}{2\sigma^2} \right] \]</span></p>
<p>con los par√°metros:</p>
<p><span class="math display">\[ L = 1, \quad x_o= 0.5, \quad \sigma = 0.1 \]</span></p>
<p>Esto representa una distribuci√≥n de carga continua, suave y localizada en (<span class="math inline">\(x_o\)</span>).</p>
<p>La soluci√≥n <span class="math inline">\(V(x)\)</span> representa el potencial producido por esa carga, el cual deber√≠a alcanzar su m√°ximo en (<span class="math inline">\(x_o\)</span>) y anularse en los bordes, <a href="https://dn390.user.srcf.net/files/waffle/IA/Vector_Calculus/poisson.pdf">M√°s informaci√≥n</a>.</p>
<p>Este problema admite una soluci√≥n anal√≠tica utilizando la funci√≥n de Green asociada al operador Laplaciano unidimensional con densidad de carga <span class="math inline">\(\rho(x)\)</span>: <span class="math display">\[V(x) = \int_0^L G(x, x') \rho(x') dx'\]</span> donde la funci√≥n de Green $ G(x, x‚Äô)$ para condiciones de borde de Dirichlet es: <span class="math display">\[\begin{equation}
G(x, x') = \begin{cases} \frac{x(L - x')}{L}, &amp; x &lt; x' \\ \frac{x'(L - x)}{L}, &amp; x &gt; x' \end{cases}
\end{equation}\]</span></p>
<p>En la pr√°ctica, esta integral puede evaluarse f√°cilmente de forma num√©rica utilizando NumPy.</p>
<div id="86f1e083" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fuente Gaussiana rho(x)</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rho_np(x, xcenter<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> xcenter) <span class="op">/</span> sigma) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Funci√≥n de Green 1D con condiciones de borde tipo Dirichlet en el rango x\in[0,L]</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># G(x,x') = x (L - x') / L   if x &lt; x'</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">#         = x' (L - x) / L   if x &gt;= x'</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> green_mat(x, xp):</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x: posiciones 1D con shape (M,) ; xp: posiciones primas 1D con shape (N,)</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># retorna una matriz (M,N)  G_ij = G(x[i], xp[j])</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x[:, <span class="va">None</span>]   <span class="co"># shape (M,1)</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    xp <span class="op">=</span> xp[<span class="va">None</span>, :] <span class="co"># shape (1,N)</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> (x <span class="op">&lt;</span> xp)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> np.where(mask, x <span class="op">*</span> (L <span class="op">-</span> xp) <span class="op">/</span> L, xp <span class="op">*</span> (L <span class="op">-</span> x) <span class="op">/</span> L)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> G</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Paramemtros iniciales</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>xo <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Creamos un grid para evaluar la soluci√≥n</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(<span class="fl">0.</span>, L, N) <span class="co"># puntos de evaluaci√≥n x. shape:(N,)</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>xsp <span class="op">=</span> np.linspace(<span class="fl">0.</span>, L, N)            <span class="co"># puntos de integraci√≥n x'. shape:(N,)</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculamos la soluci√≥n anal√≠tica V(x) = \int_0^L G(x,x') rho(x') dx'</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>rho_vals <span class="op">=</span> rho_np(xsp,xo,sigma)  <span class="co"># # note que estamos evaluando sobre x' para realizar la integral </span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> green_mat(xs, xsp) <span class="co"># shape (N, N)</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="co"># podemos usar el metodo del trapecio sobre x'</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>V_analytic <span class="op">=</span> G.dot(rho_vals)<span class="op">*</span>(xsp[<span class="dv">1</span>] <span class="op">-</span> xsp[<span class="dv">0</span>])  <span class="co"># shape (N,)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="220d6fe8" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficamos la soluci√≥mn</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">3</span>))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(xs, rho_vals)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'rho(x)'</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(xs, V_analytic)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Potential V(x)'</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="PINN_notebook_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Ahora vamos a implementar PINNs para resolver este problema!</p>
<div style="border-left: 4px solid #4caf50; padding: 8px;">
<p>üîÑ <strong>¬øQu√© cambia respecto al aprendizaje supervisado?</strong></p>
<p>En el aprendizaje supervisado tradicional, entrenamos el modelo usando pares de datos conocidos <span class="math inline">\({x_i, y_i}\)</span>, donde los valores <span class="math inline">\(y_i\)</span> sirven como <em>targets</em> o valores objetivo.</p>
<p>El modelo aprende a aproximar la relaci√≥n <span class="math inline">\(x_i \mapsto y_i\)</span>.</p>
<p>en una PINN pedimos adicionalmente:</p>
<ul>
<li>Usamos conocimiento f√≠sico expresado en forma de ecuaciones diferenciales (PDEs) que el modelo debe satisfacer.</li>
<li>Tambi√©n es posible usar formulaciones variacionales, donde el entrenamiento se basa en minimizar un funcional de energ√≠a.</li>
</ul>
</div>
<p>Una PINN transforma un problema de ajuste de datos en un problema de consistencia f√≠sica, donde buscamos una funci√≥n <span class="math inline">\(\hat{y}_\theta(x)\)</span> que minimice el residuo de la PDE. Esto permite entrenar incluso con pocos datos, siempre que tengamos una buena descripci√≥n matem√°tica del sistema.</p>
<p>Para entrenar nuestra PINN, vamos a necesitar:</p>
<ul>
<li><p>Un MLP para <span class="math inline">\(V_\theta(x)\)</span>, donde <span class="math inline">\(\theta\)</span> son los parametros de la red.</p></li>
<li><p>Definir la funci√≥n ‚Äúresiduo‚Äù de la PDE <span class="math display">\[R_\theta \equiv V_\theta''(x) + \rho(x)\]</span></p></li>
<li><p>Incluir las condiciones de Borde <span class="math display">\[V_\theta(0) = 0, \quad V_\theta(L) = 0 \]</span></p></li>
</ul>
<p>Luego, la funci√≥n de coste total a minimizar es: <span class="math display">\[\mathcal{L} = \text{MSE}\left(R_\theta \right) + \text{MSE}(\text{B.C.})\]</span></p>
<p>Expl√≠citamente:</p>
<p><span class="math display">\[\mathcal{L} = \mathcal{L}_{PDE}+\mathcal{L}_{\rm{B.C.}} = \frac{1}{N}\sum_{i}^N\left|\left.\frac{d^2V_\theta (x)}{dx^2}\right|_{x_i} + \rho(x_i)\right|^2 + \rm{w}_b \left[ |V_\theta(0)|^2 + |V_\theta(L)|^2 \right]\]</span></p>
<p>El par√°metro <span class="math inline">\(\rm{w}_b\)</span> cumple un rol crucial: es un factor de ponderaci√≥n que equilibra la importancia entre el cumplimiento de la PDE y las condiciones de borde</p>
<ul>
<li>Si el t√©rmino de la PDE domina (valores grandes en comparaci√≥n con las BC), el modelo puede ignorar las condiciones de borde.</li>
<li>Si el t√©rmino de borde domina, el modelo podr√≠a ajustar las condiciones pero no resolver correctamente la PDE en el interior.</li>
</ul>
<blockquote class="blockquote">
<p>Por eso, <span class="math inline">\(\rm{w}_b\)</span> controla el balance entre precisi√≥n f√≠sica y consistencia en los bordes.</p>
</blockquote>
<div id="b44970d1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    torch.set_default_device(<span class="st">"cuda"</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PoissonLoss(nn.Module):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, L<span class="op">=</span><span class="fl">1.0</span>, w_b<span class="op">=</span><span class="fl">1e3</span>, xo<span class="op">=</span><span class="fl">0.5</span>, sigma<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> L</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_b <span class="op">=</span> w_b</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.xo <span class="op">=</span> xo</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigma <span class="op">=</span> sigma</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> rho(<span class="va">self</span>, x):</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>((x <span class="op">-</span> <span class="va">self</span>.xo)<span class="op">/</span><span class="va">self</span>.sigma)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, model, x_dom, xb0, xbL):</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        x_dom.requires_grad_(<span class="va">True</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> model(x_dom)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        dV_dx <span class="op">=</span> torch.autograd.grad(V, x_dom,</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>                                    grad_outputs<span class="op">=</span>torch.ones_like(V),</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>                                    create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        d2V_dx2 <span class="op">=</span> torch.autograd.grad(dV_dx, x_dom,</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>                                      grad_outputs<span class="op">=</span>torch.ones_like(dV_dx),</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>                                      create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        rho <span class="op">=</span> <span class="va">self</span>.rho(x_dom)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        pde_res <span class="op">=</span> d2V_dx2 <span class="op">+</span> rho</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        pde_loss <span class="op">=</span> torch.mean(pde_res<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluamos en el borde</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        V0 <span class="op">=</span> model(xb0)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        VL <span class="op">=</span> model(xbL)</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        bc_loss <span class="op">=</span> torch.mean(V0<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> VL<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pde_loss <span class="op">+</span> <span class="va">self</span>.w_b <span class="op">*</span> bc_loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Reciclamos algunas de las funciones del problema anterior aqu√≠:</p>
<div id="41f2239e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MLP, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>)          <span class="co"># Fully connected layer: 1 input, 10 hidden units</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.functional.tanh <span class="co"># Funci√≥n de activaci√≥n</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)          <span class="co"># Fully connected layer: 10 hidden units, 1 output</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.activation(x)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(puntos_interior, puntos_borde_0, puntos_borde_L, model, optimizer, criterion):</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(modelo, puntos_interior, puntos_borde_0, puntos_borde_L)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item()</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, x_test):</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(x_test).cpu().numpy()</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_pred.squeeze()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="20492b65" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>modelo <span class="op">=</span> MLP()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>criterio <span class="op">=</span> PoissonLoss(L<span class="op">=</span>L)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(modelo.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>num_epocas <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>n_dom <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoca <span class="kw">in</span> <span class="bu">range</span>(num_epocas):</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    x_dom <span class="op">=</span> torch.rand(n_dom,<span class="dv">1</span>)<span class="op">*</span>L</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    x_en_0 <span class="op">=</span> torch.zeros((<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    x_en_L <span class="op">=</span> torch.full((<span class="dv">1</span>,<span class="dv">1</span>), L)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> train_epoch(x_dom, x_en_0, x_en_L, modelo, optimizer, criterio)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># # Train_epoch</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># modelo.train()</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optimizer.zero_grad()</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss = criterio(modelo, x_dom, x_en_0, x_en_L)</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss.backward()</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optimizer.step()</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># #</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoca<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch [</span><span class="sc">{</span>epoca<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epocas<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1000/10000, Loss = 1.47e-01
Epoch 2000/10000, Loss = 1.37e-01
Epoch 3000/10000, Loss = 1.22e-01
Epoch 4000/10000, Loss = 1.06e-01
Epoch 5000/10000, Loss = 8.13e-02
Epoch 6000/10000, Loss = 4.52e-02
Epoch 7000/10000, Loss = 3.49e-02
Epoch 8000/10000, Loss = 3.10e-02
Epoch 9000/10000, Loss = 2.25e-02
Epoch 10000/10000, Loss = 1.25e-02</code></pre>
</div>
</div>
<div id="4faf8466" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> torch.tensor(xs[:,<span class="va">None</span>], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>V_pred <span class="op">=</span> evaluate(modelo, x_test)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, V_analytic, <span class="st">'k--'</span>, label<span class="op">=</span><span class="st">'Anal√≠tico'</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, V_pred, <span class="st">'r'</span>, label<span class="op">=</span><span class="st">'PINN'</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'V(x)'</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> plt.tight_layout()<span class="op">;</span> plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="PINN_notebook_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="extras" class="level3" data-number="2.0.1">
<h3 data-number="2.0.1" class="anchored" data-anchor-id="extras"><span class="header-section-number">2.0.1</span> Extras:</h3>
<p>Una part√≠cula cargada localizada en x_0 se modela como una distribuci√≥n delta de Dirac: <span class="math inline">\(\rho(x) = \delta_D(x - x_0)\)</span></p>
<p>Para nuestro problema, esta se puede obtener como el l√≠mite de la fuente Gaussiana:</p>
<p><span class="math display">\[\rho(x) = \exp\left[ -\frac{(x - x_0)^2}{2\sigma^2} \right] \quad \text{cuando } \sigma \to 0 \]</span></p>
<ul>
<li>Ajuste el c√≥digo anterior para resolver el problema de la particula cargada.</li>
<li>C√≥mo se comporta nuestro modelo para este tipo de densidad?</li>
</ul>
</section>
</section>
<section id="ejemplo-guiado-oscilador-arm√≥nico-en-1d" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> <span style="color:#1f77b4;">Ejemplo guiado:</span> Oscilador arm√≥nico en 1D</h1>
<p>El oscilador arm√≥nico amortiguado en 1D:</p>
<p><span class="math display">\[m \dfrac{d^2 x}{d t^2} + \mu \dfrac{d x}{d t} + kx = 0~, \]</span></p>
<p>con condiciones iniciales: <span class="math display">\[x(0) = 1, \quad \dfrac{d x}{d t} = 0\]</span></p>
<p>Nos enfocaremos en resolver el problema en el r√©gimen subamortiguado, es decir, cuando</p>
<p><span class="math display">\[
\delta &lt; \omega_0 \rightarrow \quad \delta = \dfrac{\mu}{2m},\quad\omega_0 = \sqrt{\dfrac{k}{m}}.
\]</span></p>
<p>Esto tiene la siguiente soluci√≥n exacta:</p>
<p><span class="math display">\[
x(t) = e^{-\delta t}(2 A \cos(\phi + \omega t)),\quad\mathrm{con}\quad \omega=\sqrt{\omega_0^2 - \delta^2}.
\]</span></p>
<p><a href="https://la.mathworks.com/discovery/physics-informed-neural-networks.html">M√°s informaci√≥n</a></p>
<hr>
<p>Vamos a fijar algunas constantes libres del problema</p>
<p><span class="math display">\[m=1,\, \mu=4, k=400 \, \Rightarrow\, \delta=2&lt;\omega_0=400\]</span></p>
<p>y consideramos el intervalo de tiempo</p>
<p><span class="math display">\[t\in[0,1]\]</span></p>
<p>Ahora la loss tendremos dos contribuciones:</p>
<ol type="1">
<li>El residuo de la PDE: <span class="math inline">\(R_\theta \equiv m\ddot{x}+\mu\dot{x}+kx\)</span></li>
<li>La condicion iniciales: <span class="math inline">\(x(t=0)=1\)</span>, y <span class="math inline">\(\dot{x}(t=0)=0\)</span></li>
</ol>
<div id="079e9240" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DampedOscillatorLoss(nn.Module):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, m<span class="op">=</span><span class="fl">1.0</span>, mu<span class="op">=</span><span class="fl">4.</span>, k<span class="op">=</span><span class="fl">400.0</span>, w_b<span class="op">=</span><span class="fl">1e3</span>):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.m, <span class="va">self</span>.mu, <span class="va">self</span>.k <span class="op">=</span> m, mu, k</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_b <span class="op">=</span> w_b</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, model, t_dom, t0):</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> model(t_dom)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        dx_dt <span class="op">=</span> torch.autograd.grad(x, t_dom,</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>                                    grad_outputs<span class="op">=</span>torch.ones_like(x),</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>                                    create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        d2x_dt2 <span class="op">=</span> torch.autograd.grad(dx_dt, t_dom,</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>                                      grad_outputs<span class="op">=</span>torch.ones_like(dx_dt),</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>                                      create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residue de la PDE</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        pde_residual <span class="op">=</span> <span class="va">self</span>.m <span class="op">*</span> d2x_dt2 <span class="op">+</span> <span class="va">self</span>.mu <span class="op">*</span> dx_dt <span class="op">+</span> <span class="va">self</span>.k <span class="op">*</span> x</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        pde_loss <span class="op">=</span> torch.mean(pde_residual<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Condiciones iniciales</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        x0 <span class="op">=</span> model(t0)</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        dx_dt0 <span class="op">=</span> torch.autograd.grad(x0, t0,</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>                                     grad_outputs<span class="op">=</span>torch.ones_like(x0),</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>                                     create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>        ic_x_en_t0 <span class="op">=</span> torch.mean(x0 <span class="op">-</span> <span class="fl">1.0</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>        ic_vel_en_t0 <span class="op">=</span> torch.mean(dx_dt0<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pde_loss <span class="op">+</span> <span class="va">self</span>.w_b <span class="op">*</span> (ic_x_en_t0<span class="op">+</span>ic_vel_en_t0)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="07393bbf" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1</span>, <span class="dv">64</span>),</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">64</span>, <span class="dv">64</span>),</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, t):</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(t)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c9c58207" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># domain and initial condition points</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>t_dom <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">200</span>, requires_grad<span class="op">=</span><span class="va">True</span>)[:,<span class="va">None</span>]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>t0 <span class="op">=</span> torch.tensor([[<span class="fl">0.0</span>]], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MLP()</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> DampedOscillatorLoss()</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(model, t_dom, t0)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: Loss = </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.2e}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1000: Loss = 9.65e+02
Epoch 2000: Loss = 9.50e+02
Epoch 3000: Loss = 6.89e+02
Epoch 4000: Loss = 3.73e+02
Epoch 5000: Loss = 2.41e+02
Epoch 6000: Loss = 1.64e+02
Epoch 7000: Loss = 1.25e+02
Epoch 8000: Loss = 1.06e+02
Epoch 9000: Loss = 7.06e+01
Epoch 10000: Loss = 3.03e+01</code></pre>
</div>
</div>
<div id="1c2cb6ad" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    t_test <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">200</span>)[:,<span class="va">None</span>]</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    x_pred <span class="op">=</span> model(t_test).cpu().numpy().squeeze()</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> x_analitico(t, m<span class="op">=</span><span class="dv">1</span>, mu<span class="op">=</span><span class="dv">4</span>, k<span class="op">=</span><span class="fl">400.</span>):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> mu<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    omega0 <span class="op">=</span> np.sqrt(k<span class="op">/</span>m)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    omega <span class="op">=</span> np.sqrt(omega0<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> delta<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    A, B <span class="op">=</span> <span class="fl">1.0</span>, delta<span class="op">/</span>omega</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>delta<span class="op">*</span>t)<span class="op">*</span>(A<span class="op">*</span>np.cos(omega<span class="op">*</span>t) <span class="op">+</span> B<span class="op">*</span>np.sin(omega<span class="op">*</span>t))</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> t_test.cpu().numpy().squeeze()</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>))</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>plt.plot(t, x_analitico(t), <span class="st">'k--'</span>, label<span class="op">=</span><span class="st">'Anal√≠tico'</span>)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>plt.plot(t, x_pred, <span class="st">'r'</span>, label<span class="op">=</span><span class="st">'PINN'</span>)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'t'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'x(t)'</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> plt.tight_layout()<span class="op">;</span> plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="PINN_notebook_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
<section id="extra" class="level3" data-number="3.0.1">
<h3 data-number="3.0.1" class="anchored" data-anchor-id="extra"><span class="header-section-number">3.0.1</span> Extra:</h3>
<ul>
<li>Intenta redefinir el MLP para que la salida del modelo tenga la siguiente forma:</li>
</ul>
<p><span class="math display">\[x(t)= e^{‚àí\delta t} N_\theta(t)\]</span></p>
<p>donde:</p>
<ul>
<li><span class="math inline">\(N_\theta\)</span> es el valor producido por el modelo secuencial (MLP) para una entrada <span class="math inline">\((t)\)</span>.</li>
<li><span class="math inline">\(\delta = \mu/2m\)</span> para este ejercicio lo podemos dar por conocido.
<ul>
<li>Sin embargo, en muchos casos no conocemos su valor exacto, por lo que en rigor deber√≠a tratarse como un par√°metro adicional a aprender por la red!</li>
<li>Como segundo ejercicio, modifica el c√≥digo para que el MLP tambi√©n aprenda <span class="math inline">\(\delta\)</span>.</li>
</ul></li>
</ul>
<p>¬øC√≥mo ayuda esta reparametrizaci√≥n de la red? ¬øToma m√°s o menos √©pocas en converger la soluci√≥n? ¬øMejora el ajuste entre el valor anal√≠tico y la soluci√≥n de nuestro modelo? ¬øVale la pena el trabajo extra?</p>
</section>
</section>
<section id="ejemplo-guiado-ecuaci√≥n-de-helmholtz" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> <span style="color:#1f77b4;">Ejemplo guiado:</span> Ecuaci√≥n de Helmholtz</h1>
<p>Modelamos una cavidad electromagn√©tica unidimensional de longitud (L) con paredes perfectamente conductoras.</p>
<p>Ecuaci√≥n diferencial parcial (EDP):</p>
<p><span class="math display">\[\frac{d^2 E(x)}{dx^2} + k^2 E(x) = 0, \quad x \in \Omega = [0, L]\]</span></p>
<p>Condiciones de borde:</p>
<p><span class="math display">\[ E(0) = 0, \quad E(L) = 0 \]</span></p>
<p>Soluci√≥n anal√≠tica:</p>
<p><span class="math display">\[ E_n(x) = A \sin\left( \frac{n \pi x}{L} \right), \quad k_n = \frac{n \pi}{L}\]</span></p>
<p>Entrenaremos una PINN para aprender <span class="math inline">\(E(x)\)</span> para un valor fijo de <span class="math inline">\(k\)</span>, por ejemplo <span class="math inline">\(n=1,\,L=1,\, \rightarrow k = \pi\)</span>, es decir, el modo fundamental.</p>
<p><a href="https://en.wikipedia.org/wiki/Helmholtz_equation">M√°s informaci√≥n</a></p>
<hr>
<p>La PDE define la forma del modo, pero la amplitud es arbitraria (<span class="math inline">\(A\)</span>). Como consecuencia, la red puede aprender la soluci√≥n trivial <span class="math inline">\(E(x)=0\)</span>, la cual, por supuesto, tambi√©n es soluci√≥n de esta ecuaci√≥n diferencial. Por lo tanto, a veces necesitamos incluir m√°s informaci√≥n en nuestra funci√≥n de costo para evitar que se minimice hacia la soluci√≥n trivial. Una opci√≥n es normalizarla exigiendo que la ‚Äúenerg√≠a‚Äù (o norma <span class="math inline">\(L^2\)</span>) sea igual a <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[\int_0^L E^2(x) dx = 1\]</span></p>
<p>Entonces, en forma discreta (usando integraci√≥n a la monte carlo):</p>
<p><span class="math display">\[\mathcal{L}_{\text{norm}} = \left( \frac{1}{N} \sum_i E^2(x_i) - 1 \right)^2\]</span></p>
<p>Luego, la p√©rdida total se convierte en:</p>
<p><span class="math display">\[\mathcal{L} = \mathcal{L}_{\text{PDE}} + \mathcal{L}_{\text{norm}} + \mathcal{L}_\text{B.C}\]</span></p>
<div id="c3df60cf" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HelmholtzLoss(nn.Module):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, w_b<span class="op">=</span><span class="dv">1</span>, n<span class="op">=</span><span class="dv">1</span>, L<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(HelmholtzLoss, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_b <span class="op">=</span> w_b</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> n<span class="op">*</span>torch.pi <span class="op">/</span> L</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, model, domain_samples):</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ----- PDE term -----</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        E <span class="op">=</span> model(domain_samples)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First derivative dE/dx</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        dE_dx <span class="op">=</span> torch.autograd.grad(</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            outputs<span class="op">=</span>E,</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>            inputs<span class="op">=</span>domain_samples,</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>            grad_outputs<span class="op">=</span>torch.ones_like(E),</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            create_graph<span class="op">=</span><span class="va">True</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>        )[<span class="dv">0</span>]</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second derivative d^2E/dx^2</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        d2E_dx2 <span class="op">=</span> torch.autograd.grad(</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>            outputs<span class="op">=</span>dE_dx,</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>            inputs<span class="op">=</span>domain_samples,</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>            grad_outputs<span class="op">=</span>torch.ones_like(dE_dx),</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>            create_graph<span class="op">=</span><span class="va">True</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>        )[<span class="dv">0</span>]</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual of Helmholtz eqn: E'' + k^2E = 0</span></span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>        pde_residual <span class="op">=</span> d2E_dx2 <span class="op">+</span> <span class="va">self</span>.k <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> E</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        pde_loss <span class="op">=</span> torch.mean(pde_residual <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>        norm <span class="op">=</span> torch.mean(E<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>        norm_loss <span class="op">=</span> (norm <span class="op">-</span> <span class="fl">1.0</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pde_loss <span class="op">+</span> <span class="va">self</span>.w_b <span class="op">*</span> norm_loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Notemos que esta vez no incluimos los t√©rminos de borde en la Loss, ya que vamos a introducir otra manera de imponer condiciones de borde: &gt;simplemente multiplicar la salida de la red por una funci√≥n gen√©rica que se anule en el borde.</p>
<p>Por ejemplo, para condiciones de borde tipo Dirichlet, podemos imponer estas condiciones directamente sobre la arquitectura de la red. En lugar de aprender la funci√≥n en el borde, definimos:</p>
<p><span class="math display">\[E_\theta(x) = f|_{\partial\Omega} \cdot N\theta(x)\]</span></p>
<p>donde:</p>
<ul>
<li><p><span class="math inline">\(N_\theta(x)\)</span> es el valor producido por el modelo secuencial (MLP) para una entrada <span class="math inline">\(x\)</span>,</p></li>
<li><p><span class="math inline">\(f|_{\partial\Omega}\)</span> es una funci√≥n particular que satisface las condiciones de borde.</p></li>
</ul>
<p>En este caso espec√≠fico, tomando <span class="math inline">\(f|_{\partial\Omega} = x(L - x)\)</span> garantiza que ( $E_(0) = E_(L) = 0 $), cumpliendo as√≠ las condiciones de Dirichlet deseadas.</p>
<p>Esto nos permite ahorrarnos el termino de borde en la loss!</p>
<div id="0216847b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HelmholtzMLP(nn.Module):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, L):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> L</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>)          <span class="co"># Fully connected layer: 1 input feature, 10 hidden units</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.functional.tanh <span class="co"># Funci√≥n de activaci√≥n</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)          <span class="co"># Fully connected layer: 10 hidden units, 1 output</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        E_x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        E_x <span class="op">=</span> <span class="va">self</span>.activation(E_x)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        E_x <span class="op">=</span> <span class="va">self</span>.fc2(E_x)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        E_x <span class="op">=</span> x <span class="op">*</span> (<span class="va">self</span>.L <span class="op">-</span> x) <span class="op">*</span> E_x   <span class="co"># Enforces E(0)=E(L)=0</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> E_x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>reciclamos algunas funciones anteriores,</p>
<div id="7de3a564" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, optimizer, criterion, inputs):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(model, inputs)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item()</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, x_test):</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(x_test).cpu().numpy()</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_pred.squeeze()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>y entranamos nuestro modelo</p>
<div id="828f8def" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>box_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>num_points <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>modelo         <span class="op">=</span> HelmholtzMLP(box_size)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>function_coste <span class="op">=</span> HelmholtzLoss(w_b<span class="op">=</span><span class="dv">1</span>, n<span class="op">=</span><span class="dv">1</span>, L<span class="op">=</span>box_size)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>optimizador    <span class="op">=</span> optim.Adam(modelo.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> torch.rand(num_points, <span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">True</span>) <span class="op">*</span> box_size</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> train_epoch(modelo, optimizador, function_coste, inputs)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>loss<span class="sc">:.4e}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1000/10000], Loss: 9.7957e-01
Epoch [2000/10000], Loss: 1.0372e-03
Epoch [3000/10000], Loss: 7.4442e-04
Epoch [4000/10000], Loss: 4.5278e-04
Epoch [5000/10000], Loss: 2.9588e-04
Epoch [6000/10000], Loss: 9.4307e-05
Epoch [7000/10000], Loss: 1.4104e-04
Epoch [8000/10000], Loss: 6.5982e-05
Epoch [9000/10000], Loss: 1.8265e-04
Epoch [10000/10000], Loss: 2.3351e-03</code></pre>
</div>
</div>
<div id="8e16173a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> torch.linspace(<span class="dv">0</span>, box_size, <span class="dv">200</span>)[:,<span class="va">None</span>]</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>E_pred <span class="op">=</span> evaluate(modelo, x_test)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>E_true <span class="op">=</span> np.sqrt(<span class="dv">2</span><span class="op">/</span>box_size)<span class="op">*</span>np.sin(n<span class="op">*</span>np.pi<span class="op">*</span>x_test.cpu().numpy() <span class="op">/</span> box_size)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x_test.squeeze().cpu().numpy() , E_pred.squeeze(), <span class="st">'red'</span>, label<span class="op">=</span><span class="st">'PINN'</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x_test.squeeze().cpu().numpy() , E_true.squeeze(), <span class="st">'--'</span>, color<span class="op">=</span><span class="st">'k'</span>, label<span class="op">=</span><span class="st">'Anal√≠tico'</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'E(x)'</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="PINN_notebook_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="case-of-study-poisson-en-2d" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> <span style="color:#1f77b4;">Case of study:</span> Poisson en 2D</h1>
<p>En el siguiente problema consideramos un sistema con una fuente interna distribuida de forma sinusoidal en ambas direcciones espaciales, descrito por la siguiente ecuaci√≥n de Poisson:</p>
<p><span class="math display">\[-\Delta u(x) = 8 \pi^2 \sin(2\pi x) \sin(2\pi y)\]</span></p>
<p>en el dominio cuadrado <span class="math inline">\(x \in \Omega = (-1,1)\times(-1,1)\)</span>.</p>
<p>Se imponen condiciones de borde de tipo Dirichlet, especificando que <span class="math display">\[ u(x) = 0, \in \partial\Omega .\]</span></p>
<p>La soluci√≥n anal√≠tica de este problema es:</p>
<p><span class="math display">\[ u(x) = \sin(2\pi x) \sin(2\pi y). \]</span></p>
<div id="0c2a5d99" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tu soluci√≥n</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="case-of-study-laplace-en-2d-con-una-singularidad-de-cu√±a" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> <span style="color:#1f77b4;">Case of study:</span> Laplace en 2D con una singularidad de cu√±a</h1>
<p>Consideremos el siguiente problema de Dirichlet para la ecuaci√≥n de Laplace en el dominio plano <span class="math inline">\(\Omega\)</span>,</p>
<p><span class="math display">\[ \Delta u(x) = 0,  \in \Omega\]</span></p>
<p>con condici√≥n de borde dada por:</p>
<p><span class="math display">\[ u(x) = u(r,\theta) = r^{1/2} \sin{\left( \frac{\theta}{2} \right)}, \in \partial\Omega \]</span></p>
<p>donde el dominio se define como $= (-1,1)(-1,1)[0,1) {0} $.</p>
<p>Este dominio corresponde a un cuadrado con una hendidura (o corte) a lo largo del eje <span class="math inline">\(x\)</span> positivo, desde el origen hasta el punto <span class="math inline">\((1,0)\)</span>, lo que introduce una singularidad tipo cu√±a en el origen.</p>
<p>La soluci√≥n anal√≠tica de este problema es:</p>
<p><span class="math display">\[ u(x) = r^{1/2} \sin{\left( \frac{\theta}{2} \right)}. \]</span></p>
<p>donde <span class="math inline">\((r,\theta )\)</span> son las coordenadas polares centradas en el origen.</p>
<div id="af98347a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tu soluci√≥n</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/jcallesh\.github\.io\/PINN_escuela_2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>